{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN_gy_review",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungGaLee/AI_Study/blob/master/%5B%EC%8B%A4%EC%8A%B5_4%5D%20MNIST_CNN_(%EB%B3%80%ED%99%98%ED%95%A8%EC%88%98%EC%B6%94%EA%B0%80)/MNIST_CNN_gy_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_a2gtdkAmE8",
        "colab_type": "code",
        "outputId": "8014842f-dacd-4a4c-e8dc-3e4a4106f5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#코랩에서 돌아가지 않는 코드입니다... 어떤 에러인지도 안뜨고... 세션이 다운되고 비정상 종료됩니다...\n",
        "# --> data를 한 번에 다 load하는 코드 존재 ==> 서버가 감당하지 못하고 세션 다운 -> DataLoader와 batch를 사용하여 이를 해결\n",
        "\n",
        "#저번 MNIST_CNN_import : 이거 다 없어도 될거 같음\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#변환 코드_import \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import time\n",
        "\n",
        "#클래스 따로 위쪽으로 뺌.\n",
        "  #변환코드클래스\n",
        "class MyDataset(Dataset):\n",
        "  \n",
        "    def __init__(self, image_path, label_path):\n",
        "        self.image_data = torch.from_numpy(self.read_image(image_path)) #numpy배열 > tensor로 바꿔줌\n",
        "        self.label_data = torch.from_numpy(self.read_label(label_path)).long()\n",
        "        self.len = self.label_data.size()[0]\n",
        "        #__init__부분에서 read_image함수를 불러오고 read_image에서 read함수를 불어오기 때문에\n",
        "        #따로 사용하는 것이 아니라 그냥 MyDataset에 경로지정해주면 알아서 진행\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.image_data[item].float() ,self.label_data[item]\n",
        "\n",
        "\n",
        "    #read라는 함수를 돌릴때마다 앞의 4자리 읽어서 출력하는거 28 또는 0으로 되더라\n",
        "    def read(self, data_file): # 앞의 4byte를 읽은 후 표준 형식으로 바꿔 출력    ????numpy????\n",
        "        dt = np.dtype(np.uint32).newbyteorder('>')#np.dtype: ,numpy.dtype.newbyteorder(uint32)부호화되지 않은 32비트=4바이트 순서로 있는 원래 자료를 해석할 수 있도록 배열 dtype에 바이트 순서 정보를 변경하는 것\n",
        "        # >u4 : 4글자 유니코드 문자열\n",
        "        #what = np.frombuffer(data_file.read(4), dt)\n",
        "        #print(what)\n",
        "        #print(what[0])\n",
        "\n",
        "        return np.frombuffer(data_file.read(4), dt)[0] #np.frombuffer( 바꾸고 싶은 bytes , dtype = <자료형>)\n",
        "\n",
        "    def read_image(self, image_path):\n",
        "        image_file = open(image_path, 'rb')\n",
        "        # <_io.BufferedReader name='/content/drive/My Drive/MNIST_byte/t10k-images.idx3-ubyte'>\n",
        "\n",
        "        image_file.read(4) # 처음 4byte는 데이터가 MNIST라는것을 의미\n",
        "        '<--- 이 부분은 읽은 후에 저장되지 않고 사라진다.'\n",
        "        # read()함수가 실행되는것이 아님....이거 뭐야? 그냥 네개를 읽어라. <-- 4개를 읽고 버리는 것\n",
        "        # read() : 파일 전체의 내용을 하나의 문자열로 읽어온다.그냥 내장함수인듯...??\n",
        "        #b\"\\x00\\x00'\\x10\"\n",
        "        \n",
        "\n",
        "        \n",
        "        #계속 같은 ubyte파일을 넣어줌\n",
        "        #앞에 이게 파일에서 얼마로 이루어졌다 하는 정보인듯함\n",
        "        num_images = self.read(image_file) #10000 : 처음4개 \n",
        "        ' < -- 4개를 버리지 않고 정보를 저장'\n",
        "        rows = self.read(image_file) # 28 : 그다음4개\n",
        "        cols = self.read(image_file) # 28: 그다다음4개\n",
        "\n",
        "\n",
        "        buf = image_file.read(rows * cols * num_images) # 28*28*10000개(전체 수) 읽기(불러오기) = buf:test_image파일에 있는 전체 내용을 담고 있음.\n",
        "        #다 읽어왔으니까 close\n",
        "        image_file.close()\n",
        "        \n",
        "\n",
        "        data = np.frombuffer(buf, np.uint8) # ?? 왜 다시 이걸로 바꾸지..? 1바이트로   ????????? \n",
        "        ' <-- np.frombuffer는 1항에 데이터 2항에 타입으로 1항의 값을 2항의 타입으로 바꾸는 함수, uint8은 0~255 로 8bit인 int형 데이터 타입'\n",
        "        data = data.reshape(num_images, 1, rows, cols) #data : (10000,1,28,28)로 reshape \n",
        "        '<- 위에서 바꿔 읽은 1차원 데이터를 원하는 형테로 바꿔줌'\n",
        "        # print(data)#array\n",
        "        return data\n",
        "\n",
        "    def read_label(self, label_path):\n",
        "        label_file = open(label_path, 'rb')\n",
        "        label_file.read(4)# 처음 4byte는 데이터가 MNIST라는것을 의미(동일) / self없으니까 그냥 python 내장함수인 read()함수 사용 \n",
        "        '<---처음 4개 데이터는 읽어온 후 어디에도 저장하지 않고 버림'\n",
        "        num_label = self.read(label_file)#self.read니까 class에 있는 read함수 사용 /전체를 4byte로 바꿈?  ?????딱 4개만 바꾼거 아닌가????\n",
        "         '<- 그 다음 4개 데이터를 읽은 후 원하는 형태로 바꿔 num_label에 저장, 저장 된 값은 int형'\n",
        "        # print(\"여기 지나가긴 하니?\")\n",
        "        # print(num_label) #10000\n",
        "        # print(\"이거야?\")\n",
        "\n",
        "        # print(type(label_file)) #buffer reader 처리 된거\n",
        "        # print(type(num_label))\n",
        "\n",
        "\n",
        "        buf = label_file.read(num_label) # 10000개 읽어옴.\n",
        "        label_file.close()\n",
        "\n",
        "        labels = np.frombuffer(buf, np.uint8)\n",
        "        return labels\n",
        "\n",
        "\n",
        "\n",
        "# 2개 레이어 CNN 클래스\n",
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        # Final FC 7x7x64 inputs -> 10 outputs\n",
        "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:,0,:,:]\n",
        "        w,h = x.shape[1],x.shape[2]\n",
        "        x = x.view(-1,1,w,h)\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "#기본 설정\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "batch_size = 200 #ubyte를 바꾸는거니까 용량상관없으니 안써도 되지 않을까? \n",
        " '<-- ubyte를 로드하면서 numpy 이미지로 바꾸기 때문에 용량이 매우 커짐.'\n",
        "\n",
        "#경로지정\n",
        "Image_path_for_test = 't10k-images.idx3-ubyte' # test_X\n",
        "Label_path_for_test = 't10k-labels.idx1-ubyte' # test_Y\n",
        "\n",
        "image_path_for_train = 'train-images.idx3-ubyte' # train_X\n",
        "Label_path_for_train = 'train-labels.idx1-ubyte' # train_Y\n",
        "\n",
        "\n",
        "#클래스사용\n",
        "dataset_for_test = MyDataset(Image_path_for_test,Label_path_for_test)\n",
        "dataset_for_train = MyDataset(image_path_for_train,Label_path_for_train)\n",
        "model = CNN().to(device)\n",
        "\n",
        "\n",
        "'''  여기가 문제의 부분\n",
        "--> 모든 데이터를 한 번에 numpy로 바꾸어 load\n",
        "--> 그 데이터를 후에 .to(device)로 gpu에 올리게 되는데 일반 ram과 video_ram은 다름\n",
        "--> cuda는 video_ram을 사용하며 80~160만원 사이의 gpu가 보통 video_ram이 8~11GB\n",
        "\n",
        "train_X = dataset_for_train.image_data\n",
        "train_Y = dataset_for_train.label_data\n",
        "\n",
        "test_X = dataset_for_test.image_data\n",
        "test_Y = dataset_for_train.label_data\n",
        "'''\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset_for_train, batch_size=batch_size)\n",
        " '<-- DataLoader를 이용하여 원하는 batch_size만큼 load할 준비'\n",
        "num_batches = len(train_loader) // batch_size\n",
        "\n",
        "test_loader = DataLoader(dataset_for_test, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "# loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# training\n",
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(epochs): # epoch먼저\n",
        "    for batch, (train_X, train_Y) in enumerate(train_loader, 1): \n",
        "      '위에서 준비한 train_loader를 이용하여 데이터를 load'\n",
        "      avg_cost = 0\n",
        "      # image is already size of (28x28), no reshape\n",
        "      # label is not one-hot encoded\n",
        "      X = train_X.to(device)\n",
        "      Y = train_Y.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      hypothesis = model(X)\n",
        "      cost = criterion(hypothesis, Y)\n",
        "      cost.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      avg_cost += cost / num_batches\n",
        "\n",
        "      if batch % 10== 0:\n",
        "          print('[Epoch: {:>4}] batch: {:>4}  cost = {:>.9}'.format(epoch + 1, batch, cost.item()))\n",
        "\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "# with torch.no_grad():\n",
        "#     X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "#     Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "#     prediction = model(X_test)\n",
        "#     correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "#     accuracy = correct_prediction.float().mean()\n",
        "#     print('Accuracy:', accuracy.item())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning started. It takes sometime.\n",
            "[Epoch:    1] batch:   10  cost = 15.669302\n",
            "[Epoch:    1] batch:   20  cost = 3.32425976\n",
            "[Epoch:    1] batch:   30  cost = 2.49114394\n",
            "[Epoch:    1] batch:   40  cost = 1.47087097\n",
            "[Epoch:    1] batch:   50  cost = 0.378796071\n",
            "[Epoch:    1] batch:   60  cost = 1.21820712\n",
            "[Epoch:    1] batch:   70  cost = 0.934472203\n",
            "[Epoch:    1] batch:   80  cost = 0.939962626\n",
            "[Epoch:    1] batch:   90  cost = 0.384661645\n",
            "[Epoch:    1] batch:  100  cost = 0.471064121\n",
            "[Epoch:    1] batch:  110  cost = 0.332597733\n",
            "[Epoch:    1] batch:  120  cost = 0.440830737\n",
            "[Epoch:    1] batch:  130  cost = 0.339489579\n",
            "[Epoch:    1] batch:  140  cost = 0.289286047\n",
            "[Epoch:    1] batch:  150  cost = 0.742615461\n",
            "[Epoch:    1] batch:  160  cost = 0.203267246\n",
            "[Epoch:    1] batch:  170  cost = 0.115657695\n",
            "[Epoch:    1] batch:  180  cost = 0.257772982\n",
            "[Epoch:    1] batch:  190  cost = 0.27554673\n",
            "[Epoch:    1] batch:  200  cost = 0.263140023\n",
            "[Epoch:    1] batch:  210  cost = 0.291002154\n",
            "[Epoch:    1] batch:  220  cost = 0.234070972\n",
            "[Epoch:    1] batch:  230  cost = 0.340494305\n",
            "[Epoch:    1] batch:  240  cost = 0.30022496\n",
            "[Epoch:    1] batch:  250  cost = 0.579505742\n",
            "[Epoch:    1] batch:  260  cost = 0.209991723\n",
            "[Epoch:    1] batch:  270  cost = 0.347433627\n",
            "[Epoch:    1] batch:  280  cost = 0.271938086\n",
            "[Epoch:    1] batch:  290  cost = 0.0751426443\n",
            "[Epoch:    1] batch:  300  cost = 0.252707064\n",
            "[Epoch:    2] batch:   10  cost = 0.0424909256\n",
            "[Epoch:    2] batch:   20  cost = 0.0569269508\n",
            "[Epoch:    2] batch:   30  cost = 0.21469979\n",
            "[Epoch:    2] batch:   40  cost = 0.103427894\n",
            "[Epoch:    2] batch:   50  cost = 0.0411575399\n",
            "[Epoch:    2] batch:   60  cost = 0.157413378\n",
            "[Epoch:    2] batch:   70  cost = 0.142507628\n",
            "[Epoch:    2] batch:   80  cost = 0.243358195\n",
            "[Epoch:    2] batch:   90  cost = 0.149934962\n",
            "[Epoch:    2] batch:  100  cost = 0.0508298762\n",
            "[Epoch:    2] batch:  110  cost = 0.0942414179\n",
            "[Epoch:    2] batch:  120  cost = 0.26725477\n",
            "[Epoch:    2] batch:  130  cost = 0.0992539525\n",
            "[Epoch:    2] batch:  140  cost = 0.14492622\n",
            "[Epoch:    2] batch:  150  cost = 0.146627709\n",
            "[Epoch:    2] batch:  160  cost = 0.181385234\n",
            "[Epoch:    2] batch:  170  cost = 0.0669715181\n",
            "[Epoch:    2] batch:  180  cost = 0.116428092\n",
            "[Epoch:    2] batch:  190  cost = 0.0515627414\n",
            "[Epoch:    2] batch:  200  cost = 0.0863356218\n",
            "[Epoch:    2] batch:  210  cost = 0.149153054\n",
            "[Epoch:    2] batch:  220  cost = 0.112540118\n",
            "[Epoch:    2] batch:  230  cost = 0.202673689\n",
            "[Epoch:    2] batch:  240  cost = 0.136832744\n",
            "[Epoch:    2] batch:  250  cost = 0.229394257\n",
            "[Epoch:    2] batch:  260  cost = 0.178805813\n",
            "[Epoch:    2] batch:  270  cost = 0.142199501\n",
            "[Epoch:    2] batch:  280  cost = 0.0860266313\n",
            "[Epoch:    2] batch:  290  cost = 0.00936906785\n",
            "[Epoch:    2] batch:  300  cost = 0.182041928\n",
            "[Epoch:    3] batch:   10  cost = 0.0198278856\n",
            "[Epoch:    3] batch:   20  cost = 0.0332125723\n",
            "[Epoch:    3] batch:   30  cost = 0.0938965529\n",
            "[Epoch:    3] batch:   40  cost = 0.0574322417\n",
            "[Epoch:    3] batch:   50  cost = 0.00814081635\n",
            "[Epoch:    3] batch:   60  cost = 0.0924208537\n",
            "[Epoch:    3] batch:   70  cost = 0.0477110445\n",
            "[Epoch:    3] batch:   80  cost = 0.108837388\n",
            "[Epoch:    3] batch:   90  cost = 0.0972406417\n",
            "[Epoch:    3] batch:  100  cost = 0.0536781661\n",
            "[Epoch:    3] batch:  110  cost = 0.048619289\n",
            "[Epoch:    3] batch:  120  cost = 0.131123424\n",
            "[Epoch:    3] batch:  130  cost = 0.128433317\n",
            "[Epoch:    3] batch:  140  cost = 0.0757372305\n",
            "[Epoch:    3] batch:  150  cost = 0.083334215\n",
            "[Epoch:    3] batch:  160  cost = 0.0753041431\n",
            "[Epoch:    3] batch:  170  cost = 0.0193471815\n",
            "[Epoch:    3] batch:  180  cost = 0.0587527864\n",
            "[Epoch:    3] batch:  190  cost = 0.0274212174\n",
            "[Epoch:    3] batch:  200  cost = 0.109989576\n",
            "[Epoch:    3] batch:  210  cost = 0.117496699\n",
            "[Epoch:    3] batch:  220  cost = 0.0337464809\n",
            "[Epoch:    3] batch:  230  cost = 0.203910142\n",
            "[Epoch:    3] batch:  240  cost = 0.0872483552\n",
            "[Epoch:    3] batch:  250  cost = 0.195211977\n",
            "[Epoch:    3] batch:  260  cost = 0.0969955325\n",
            "[Epoch:    3] batch:  270  cost = 0.0894086808\n",
            "[Epoch:    3] batch:  280  cost = 0.0390347242\n",
            "[Epoch:    3] batch:  290  cost = 0.00558440201\n",
            "[Epoch:    3] batch:  300  cost = 0.168662414\n",
            "[Epoch:    4] batch:   10  cost = 0.0223890115\n",
            "[Epoch:    4] batch:   20  cost = 0.0100628212\n",
            "[Epoch:    4] batch:   30  cost = 0.0621309169\n",
            "[Epoch:    4] batch:   40  cost = 0.0407071076\n",
            "[Epoch:    4] batch:   50  cost = 0.0228746515\n",
            "[Epoch:    4] batch:   60  cost = 0.0863480344\n",
            "[Epoch:    4] batch:   70  cost = 0.0405890197\n",
            "[Epoch:    4] batch:   80  cost = 0.0806643516\n",
            "[Epoch:    4] batch:   90  cost = 0.087512143\n",
            "[Epoch:    4] batch:  100  cost = 0.0264923312\n",
            "[Epoch:    4] batch:  110  cost = 0.0310176797\n",
            "[Epoch:    4] batch:  120  cost = 0.0754333287\n",
            "[Epoch:    4] batch:  130  cost = 0.120652713\n",
            "[Epoch:    4] batch:  140  cost = 0.0243455451\n",
            "[Epoch:    4] batch:  150  cost = 0.0455038212\n",
            "[Epoch:    4] batch:  160  cost = 0.0677888468\n",
            "[Epoch:    4] batch:  170  cost = 0.0139784524\n",
            "[Epoch:    4] batch:  180  cost = 0.0853953734\n",
            "[Epoch:    4] batch:  190  cost = 0.0390168279\n",
            "[Epoch:    4] batch:  200  cost = 0.0602535158\n",
            "[Epoch:    4] batch:  210  cost = 0.0822306052\n",
            "[Epoch:    4] batch:  220  cost = 0.0425906926\n",
            "[Epoch:    4] batch:  230  cost = 0.215156481\n",
            "[Epoch:    4] batch:  240  cost = 0.117063038\n",
            "[Epoch:    4] batch:  250  cost = 0.104931414\n",
            "[Epoch:    4] batch:  260  cost = 0.0985698625\n",
            "[Epoch:    4] batch:  270  cost = 0.0547397956\n",
            "[Epoch:    4] batch:  280  cost = 0.0197759718\n",
            "[Epoch:    4] batch:  290  cost = 0.00855238922\n",
            "[Epoch:    4] batch:  300  cost = 0.150134847\n",
            "[Epoch:    5] batch:   10  cost = 0.0128338905\n",
            "[Epoch:    5] batch:   20  cost = 0.004883938\n",
            "[Epoch:    5] batch:   30  cost = 0.0596935861\n",
            "[Epoch:    5] batch:   40  cost = 0.031394586\n",
            "[Epoch:    5] batch:   50  cost = 0.00719825737\n",
            "[Epoch:    5] batch:   60  cost = 0.0896304771\n",
            "[Epoch:    5] batch:   70  cost = 0.0241978578\n",
            "[Epoch:    5] batch:   80  cost = 0.0639455095\n",
            "[Epoch:    5] batch:   90  cost = 0.0241460428\n",
            "[Epoch:    5] batch:  100  cost = 0.0111198165\n",
            "[Epoch:    5] batch:  110  cost = 0.0172375198\n",
            "[Epoch:    5] batch:  120  cost = 0.0854251683\n",
            "[Epoch:    5] batch:  130  cost = 0.0647339076\n",
            "[Epoch:    5] batch:  140  cost = 0.0117599629\n",
            "[Epoch:    5] batch:  150  cost = 0.0344761088\n",
            "[Epoch:    5] batch:  160  cost = 0.0137696126\n",
            "[Epoch:    5] batch:  170  cost = 0.00456279283\n",
            "[Epoch:    5] batch:  180  cost = 0.0127817392\n",
            "[Epoch:    5] batch:  190  cost = 0.0358279571\n",
            "[Epoch:    5] batch:  200  cost = 0.0141705796\n",
            "[Epoch:    5] batch:  210  cost = 0.061971765\n",
            "[Epoch:    5] batch:  220  cost = 0.0161366742\n",
            "[Epoch:    5] batch:  230  cost = 0.175812066\n",
            "[Epoch:    5] batch:  240  cost = 0.0775836259\n",
            "[Epoch:    5] batch:  250  cost = 0.116274834\n",
            "[Epoch:    5] batch:  260  cost = 0.0801033899\n",
            "[Epoch:    5] batch:  270  cost = 0.0496414453\n",
            "[Epoch:    5] batch:  280  cost = 0.0178045034\n",
            "[Epoch:    5] batch:  290  cost = 0.0148751307\n",
            "[Epoch:    5] batch:  300  cost = 0.143994287\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}